# GPU Worker Service

ุฎุฏูุฉ ูุนุงูุฌุฉ ุงูุตูุฑ ุจุงุณุชุฎุฏุงู Flux Dev + LoRA ูุฑูุน ุฌูุฏุฉ ุงูุตูุฑ.

## ๐ฏ ุงููุธุงุฆู ุงูุฑุฆูุณูุฉ

- ุฑูุน ุฌูุฏุฉ ุงูุตูุฑ ุจุงุณุชุฎุฏุงู Flux Dev + LoRA
- API endpoints ููุนุงูุฌุฉ ุงูุตูุฑ
- ูุฑุงูุจุฉ ุงุณุชุฎุฏุงู GPU ูุงูุฐุงูุฑุฉ
- ูุธุงู ููุงููุณ ุงูุฃุฏุงุก
- ุฅุฏุงุฑุฉ ุงููููุงุช ูุงูุชูุธูู ุงูุชููุงุฆู

## ๐๏ธ ุงููููู

```
gpu-worker/
โโโ app.py                 # ุงูุชุทุจูู ุงูุฑุฆูุณู
โโโ core/                  # ุงูููููุงุช ุงูุฃุณุงุณูุฉ
โ   โโโ config.py         # ุงูุฅุนุฏุงุฏุงุช
โ   โโโ models.py         # ููุงุฐุฌ ุงูุจูุงูุงุช
โ   โโโ upscaler.py       # ูุนุงูุฌ ุงูุตูุฑ
โ   โโโ monitoring.py     # ูุธุงู ุงููุฑุงูุจุฉ
โโโ utils/                 # ุงูุฃุฏูุงุช ุงููุณุงุนุฏุฉ
โ   โโโ file_handler.py   # ูุนุงูุฌ ุงููููุงุช
โ   โโโ gpu_monitor.py    # ูุฑุงูุจ GPU
โโโ tests/                 # ุงูุงุฎุชุจุงุฑุงุช
โโโ requirements.txt       # ุงููุชุทูุจุงุช
```

## ๐ ุงูุชุดุบูู

### ูุญููุงู (ููุชุทููุฑ):
```bash
cd services/gpu-worker
pip install -r requirements.txt
python app.py
```

### ุจุงุณุชุฎุฏุงู Docker:
```bash
# ูู ุงููุฌูุฏ ุงูุฑุฆูุณู
docker-compose up gpu-worker
```

## ๐ก API Endpoints

### ุงูุตุญุฉ ูุงูุญุงูุฉ
- `GET /health` - ูุญุต ุตุญุฉ ุงูุฎุฏูุฉ
- `GET /status` - ุญุงูุฉ ุงูุฎุฏูุฉ ุงูุชูุตูููุฉ
- `GET /metrics` - ููุงููุณ ุงูุฃุฏุงุก

### ูุนุงูุฌุฉ ุงูุตูุฑ
- `POST /upscale` - ุฑูุน ุฌูุฏุฉ ุงูุตูุฑุฉ

#### ูุซุงู ุนูู ุงูุงุณุชุฎุฏุงู:
```bash
curl -X POST "http://localhost:8001/upscale" \
  -F "file=@image.jpg" \
  -F "prompt=high quality, detailed, sharp"
```

## โ๏ธ ุงูุฅุนุฏุงุฏุงุช

ูููู ุชุฎุตูุต ุงูุฅุนุฏุงุฏุงุช ุนุจุฑ ูุชุบูุฑุงุช ุงูุจูุฆุฉ:

```bash
# ุฅุนุฏุงุฏุงุช ุงูุฎุงุฏู
HOST=0.0.0.0
PORT=8000
DEBUG=false

# ุฅุนุฏุงุฏุงุช ุงูููุฏูู
MODEL_PATH=/app/models
FLUX_MODEL_NAME=black-forest-labs/FLUX.1-dev
LORA_MODEL_PATH=/app/models/lora_upscaler.safetensors

# ุฅุนุฏุงุฏุงุช GPU
CUDA_DEVICE=0
ENABLE_MEMORY_EFFICIENT=true

# ุฅุนุฏุงุฏุงุช ุงููุนุงูุฌุฉ
MAX_IMAGE_SIZE=2048
MAX_FILE_SIZE=10485760
NUM_INFERENCE_STEPS=20
GUIDANCE_SCALE=7.5
```

## ๐งช ุงูุงุฎุชุจุงุฑุงุช

```bash
# ุชุดุบูู ุงูุงุฎุชุจุงุฑุงุช
cd services/gpu-worker
python -m pytest tests/ -v

# ุงุฎุชุจุงุฑ ูุญุฏุฏ
python -m pytest tests/test_basic.py::TestGPUWorkerBasic::test_health_endpoint -v
```

## ๐ ุงููุฑุงูุจุฉ

### Prometheus Metrics
- `gpu_worker_requests_total` - ุฅุฌูุงูู ุงูุทูุจุงุช
- `gpu_worker_processing_time_seconds` - ููุช ุงููุนุงูุฌุฉ
- `gpu_worker_gpu_memory_usage_bytes` - ุงุณุชุฎุฏุงู ุฐุงูุฑุฉ GPU
- `gpu_worker_images_processed_total` - ุงูุตูุฑ ุงููุนุงูุฌุฉ

### Health Checks
```bash
# ูุญุต ุณุฑูุน
curl http://localhost:8001/health

# ุญุงูุฉ ููุตูุฉ
curl http://localhost:8001/status

# ููุงููุณ ุงูุฃุฏุงุก
curl http://localhost:8001/metrics
```

## ๐ง ุงุณุชูุดุงู ุงูุฃุฎุทุงุก

### ูุดุงูู ุดุงุฆุนุฉ:

1. **CUDA ุบูุฑ ูุชููุฑ**
   ```bash
   # ุงูุชุญูู ูู CUDA
   nvidia-smi
   python -c "import torch; print(torch.cuda.is_available())"
   ```

2. **ููุงุฏ ุฐุงูุฑุฉ GPU**
   ```bash
   # ูุฑุงูุจุฉ ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ
   watch -n 1 nvidia-smi
   ```

3. **ุจุทุก ูู ุงููุนุงูุฌุฉ**
   - ุชุฃูุฏ ูู ุชูุนูู `ENABLE_MEMORY_EFFICIENT=true`
   - ููู ูู `NUM_INFERENCE_STEPS`
   - ุชุญูู ูู ุฏุฑุฌุฉ ุญุฑุงุฑุฉ GPU

### ุงูุณุฌูุงุช:
```bash
# ุนุฑุถ ุงูุณุฌูุงุช
docker-compose logs gpu-worker -f

# ุงูุณุฌูุงุช ูุน ุงูููุช
docker-compose logs gpu-worker -f --timestamps
```

## ๐ ุงูุฃูุงู

- ุงูุชุญูู ูู ุตูุบุฉ ูุญุฌู ุงููููุงุช ุงููุฑููุนุฉ
- ุชูุธูู ุชููุงุฆู ูููููุงุช ุงููุคูุชุฉ
- ุญุฏูุฏ ุฒูููุฉ ูููุนุงูุฌุฉ
- ูุฑุงูุจุฉ ุงุณุชุฎุฏุงู ุงูููุงุฑุฏ

## ๐ ุงูุฃุฏุงุก

### ุงููุชุทูุจุงุช ุงูููุตู ุจูุง:
- GPU: RTX 3090/4090 ุฃู ุฃูุถู
- VRAM: 24GB+
- RAM: 32GB+
- Storage: SSD ุณุฑูุน

### ุงูุชุญุณููุงุช:
- Memory efficient attention
- Model CPU offloading
- Attention slicing
- ุชูุธูู ุฏูุฑู ููุฐุงูุฑุฉ

## ๐ ุงูุชุทููุฑ

### ุฅุถุงูุฉ ููุฒุงุช ุฌุฏูุฏุฉ:
1. ุฃุถู endpoint ุฌุฏูุฏ ูู `app.py`
2. ุฃุถู ูููุฐุฌ ุงูุจูุงูุงุช ูู `core/models.py`
3. ุฃุถู ุงูููุทู ูู `core/upscaler.py`
4. ุฃุถู ุงุฎุชุจุงุฑุงุช ูู `tests/`

### ุฃูุถู ุงูููุงุฑุณุงุช:
- ุงุณุชุฎุฏู type hints
- ุฃุถู docstrings
- ุงูุชุจ ุงุฎุชุจุงุฑุงุช
- ุฑุงูุจ ุงูุฃุฏุงุก
- ุณุฌู ุงูุฃุฎุทุงุก ุจูุถูุญ
