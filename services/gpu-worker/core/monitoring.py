"""
Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØ§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
"""

import time
from typing import Dict
from fastapi import FastAPI, Request
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
from fastapi.responses import Response
from loguru import logger


# Prometheus metrics
REQUEST_COUNT = Counter(
    'gpu_worker_requests_total',
    'Total number of requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'gpu_worker_request_duration_seconds',
    'Request duration in seconds',
    ['method', 'endpoint']
)

PROCESSING_TIME = Histogram(
    'gpu_worker_processing_time_seconds',
    'Image processing time in seconds'
)

ACTIVE_REQUESTS = Gauge(
    'gpu_worker_active_requests',
    'Number of active requests'
)

GPU_MEMORY_USAGE = Gauge(
    'gpu_worker_gpu_memory_usage_bytes',
    'GPU memory usage in bytes'
)

GPU_UTILIZATION = Gauge(
    'gpu_worker_gpu_utilization_percent',
    'GPU utilization percentage'
)

IMAGES_PROCESSED = Counter(
    'gpu_worker_images_processed_total',
    'Total number of images processed',
    ['status']
)


class MetricsCollector:
    """Ø¬Ø§Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³"""
    
    def __init__(self):
        self.start_time = time.time()
        self.request_count = 0
        self.processing_times = []
        
    def record_request(self, method: str, endpoint: str, status: int, duration: float):
        """ØªØ³Ø¬ÙŠÙ„ Ø·Ù„Ø¨"""
        REQUEST_COUNT.labels(method=method, endpoint=endpoint, status=status).inc()
        REQUEST_DURATION.labels(method=method, endpoint=endpoint).observe(duration)
        self.request_count += 1
    
    def record_processing_time(self, duration: float):
        """ØªØ³Ø¬ÙŠÙ„ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
        PROCESSING_TIME.observe(duration)
        self.processing_times.append(duration)
        
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 100 Ù‚ÙŠØ§Ø³ ÙÙ‚Ø·
        if len(self.processing_times) > 100:
            self.processing_times = self.processing_times[-100:]
    
    def record_image_processed(self, success: bool):
        """ØªØ³Ø¬ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬Ø© ØµÙˆØ±Ø©"""
        status = "success" if success else "failed"
        IMAGES_PROCESSED.labels(status=status).inc()
    
    def update_gpu_metrics(self, memory_usage: float, utilization: float):
        """ØªØ­Ø¯ÙŠØ« Ù…Ù‚Ø§ÙŠÙŠØ³ GPU"""
        GPU_MEMORY_USAGE.set(memory_usage)
        GPU_UTILIZATION.set(utilization)
    
    def get_summary(self) -> Dict:
        """Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³"""
        uptime = time.time() - self.start_time
        avg_processing_time = (
            sum(self.processing_times) / len(self.processing_times)
            if self.processing_times else 0
        )
        
        return {
            "uptime_seconds": uptime,
            "total_requests": self.request_count,
            "average_processing_time": avg_processing_time,
            "recent_processing_times": self.processing_times[-10:] if self.processing_times else []
        }


# Global metrics collector
metrics_collector = MetricsCollector()


async def metrics_middleware(request: Request, call_next):
    """Middleware Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³"""
    start_time = time.time()
    
    # Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©
    ACTIVE_REQUESTS.inc()
    
    try:
        response = await call_next(request)
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø¯Ø© Ø§Ù„Ø·Ù„Ø¨
        duration = time.time() - start_time
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        metrics_collector.record_request(
            method=request.method,
            endpoint=request.url.path,
            status=response.status_code,
            duration=duration
        )
        
        return response
        
    except Exception as e:
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        duration = time.time() - start_time
        metrics_collector.record_request(
            method=request.method,
            endpoint=request.url.path,
            status=500,
            duration=duration
        )
        raise
    
    finally:
        # ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©
        ACTIVE_REQUESTS.dec()


def setup_monitoring(app: FastAPI):
    """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©"""
    
    # Ø¥Ø¶Ø§ÙØ© middleware
    app.middleware("http")(metrics_middleware)
    
    @app.get("/metrics")
    async def get_prometheus_metrics():
        """Prometheus metrics endpoint"""
        return Response(
            generate_latest(),
            media_type=CONTENT_TYPE_LATEST
        )
    
    @app.get("/metrics/summary")
    async def get_metrics_summary():
        """Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³"""
        return metrics_collector.get_summary()
    
    logger.info("ğŸ“Š ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©")


def record_processing_time(duration: float):
    """ØªØ³Ø¬ÙŠÙ„ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ)"""
    metrics_collector.record_processing_time(duration)


def record_image_processed(success: bool):
    """ØªØ³Ø¬ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬Ø© ØµÙˆØ±Ø© (Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ)"""
    metrics_collector.record_image_processed(success)


def update_gpu_metrics(memory_usage: float, utilization: float):
    """ØªØ­Ø¯ÙŠØ« Ù…Ù‚Ø§ÙŠÙŠØ³ GPU (Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ)"""
    metrics_collector.update_gpu_metrics(memory_usage, utilization)
