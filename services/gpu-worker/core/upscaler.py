"""
Flux Upscaler - Ù…Ø¹Ø§Ù„Ø¬ Ø±ÙØ¹ Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±
"""

import os
import time
import uuid
import asyncio
from typing import Optional, Tuple
from pathlib import Path
import torch
from PIL import Image
from loguru import logger
from diffusers import FluxPipeline
from diffusers.utils import load_image

from .config import settings, get_model_config, get_processing_config
from .models import UpscaleResponse, ProcessingStatus


class FluxUpscaler:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø±ÙØ¹ Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Flux Dev + LoRA"""
    
    def __init__(self):
        self.pipeline = None
        self.is_loaded = False
        self.device = f"cuda:{settings.CUDA_DEVICE}"
        self.model_config = get_model_config()
        self.processing_config = get_processing_config()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.total_processed = 0
        self.successful_processed = 0
        self.failed_processed = 0
        self.total_processing_time = 0.0
        
        logger.info(f"ðŸ”§ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ FluxUpscaler Ù„Ù„Ø¬Ù‡Ø§Ø²: {self.device}")
    
    async def load_models(self) -> bool:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª"""
        try:
            logger.info("ðŸ“¥ Ø¨Ø¯Ø¡ ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Flux...")
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± GPU
            if not torch.cuda.is_available():
                raise RuntimeError("CUDA ØºÙŠØ± Ù…ØªÙˆÙØ±")
            
            # ØªØ­Ù…ÙŠÙ„ Flux pipeline
            logger.info(f"ØªØ­Ù…ÙŠÙ„ {self.model_config['flux_model']}...")
            
            self.pipeline = FluxPipeline.from_pretrained(
                self.model_config['flux_model'],
                torch_dtype=torch.bfloat16,
                device_map="auto"
            )
            
            # ØªØ­Ù…ÙŠÙ„ LoRA Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹
            lora_path = self.model_config['lora_path']
            if os.path.exists(lora_path):
                logger.info(f"ØªØ­Ù…ÙŠÙ„ LoRA Ù…Ù†: {lora_path}")
                self.pipeline.load_lora_weights(lora_path)
            else:
                logger.warning(f"LoRA ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ: {lora_path}")
            
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            if self.model_config['enable_memory_efficient']:
                self.pipeline.enable_model_cpu_offload()
                self.pipeline.enable_attention_slicing()
            
            self.is_loaded = True
            logger.success("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª: {e}")
            self.is_loaded = False
            return False
    
    async def upscale_image(
        self, 
        input_path: str, 
        prompt: str,
        **kwargs
    ) -> UpscaleResponse:
        """Ø±ÙØ¹ Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±Ø©"""
        
        if not self.is_loaded:
            raise RuntimeError("Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª ØºÙŠØ± Ù…Ø­Ù…Ù„Ø©")
        
        task_id = str(uuid.uuid4())
        start_time = time.time()
        
        try:
            logger.info(f"ðŸŽ¨ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©: {task_id}")
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©
            input_image = load_image(input_path)
            original_size = input_image.size
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø©
            max_size = settings.MAX_IMAGE_SIZE
            if max(original_size) > max_size:
                # ØªØµØºÙŠØ± Ø§Ù„ØµÙˆØ±Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                ratio = max_size / max(original_size)
                new_size = (int(original_size[0] * ratio), int(original_size[1] * ratio))
                input_image = input_image.resize(new_size, Image.Resampling.LANCZOS)
                logger.info(f"ØªÙ… ØªØµØºÙŠØ± Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† {original_size} Ø¥Ù„Ù‰ {new_size}")
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
            generation_params = {
                "prompt": prompt,
                "image": input_image,
                "num_inference_steps": kwargs.get("num_inference_steps", self.processing_config["num_inference_steps"]),
                "guidance_scale": kwargs.get("guidance_scale", self.processing_config["guidance_scale"]),
                "strength": kwargs.get("strength", self.processing_config["strength"]),
                "generator": torch.Generator(device=self.device).manual_seed(kwargs.get("seed", 42))
            }
            
            # Ø¥Ø¶Ø§ÙØ© negative prompt Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹
            negative_prompt = kwargs.get("negative_prompt", self.processing_config["negative_prompt"])
            if negative_prompt:
                generation_params["negative_prompt"] = negative_prompt
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©
            logger.info("ðŸ”„ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...")
            
            with torch.inference_mode():
                result_image = self.pipeline(**generation_params).images[0]
            
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            output_path = await self._save_result(result_image, task_id)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª
            processing_time = time.time() - start_time
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.total_processed += 1
            self.successful_processed += 1
            self.total_processing_time += processing_time
            
            logger.success(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ {processing_time:.2f} Ø«Ø§Ù†ÙŠØ©")
            
            return UpscaleResponse(
                task_id=task_id,
                status=ProcessingStatus.COMPLETED,
                output_path=output_path,
                processing_time=processing_time,
                original_size=original_size,
                output_size=result_image.size,
                file_size=os.path.getsize(output_path) if os.path.exists(output_path) else None
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.total_processed += 1
            self.failed_processed += 1
            
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© {task_id}: {e}")
            
            return UpscaleResponse(
                task_id=task_id,
                status=ProcessingStatus.FAILED,
                processing_time=processing_time,
                error_message=str(e)
            )
    
    async def _save_result(self, image: Image.Image, task_id: str) -> str:
        """Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            result_dir = Path(settings.RESULT_DIR)
            result_dir.mkdir(parents=True, exist_ok=True)
            
            # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ø§Ù„Ø­ÙØ¸
            output_filename = f"upscaled_{task_id}.png"
            output_path = result_dir / output_filename
            
            # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø©
            image.save(output_path, "PNG", optimize=True)
            
            logger.info(f"ðŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ: {output_path}")
            return str(output_path)
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {e}")
            raise
    
    async def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        try:
            if self.pipeline:
                del self.pipeline
                torch.cuda.empty_cache()
                logger.info("ðŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ù…ÙˆØ§Ø±Ø¯ GPU")
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {e}")
    
    def get_stats(self) -> dict:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
        avg_time = (self.total_processing_time / self.total_processed 
                   if self.total_processed > 0 else 0)
        
        success_rate = (self.successful_processed / self.total_processed * 100 
                       if self.total_processed > 0 else 0)
        
        return {
            "total_processed": self.total_processed,
            "successful_processed": self.successful_processed,
            "failed_processed": self.failed_processed,
            "average_processing_time": avg_time,
            "success_rate": success_rate,
            "total_processing_time": self.total_processing_time
        }
